@article{SAMARASINGHE2025105613,
    title = {Neurons and neural networks to model proteins and protein networks},
    journal = {BioSystems},
    volume = {258},
    pages = {105613},
    year = {2025},
    issn = {0303-2647},
    doi = {https://doi.org/10.1016/j.biosystems.2025.105613},
    url = {https://www.sciencedirect.com/science/article/pii/S0303264725002230},
    author = {Sandhya Samarasinghe and Tran Nguyen Minh-Thai and Komal Sorthiya and Don Kulasiri},
    keywords = {Proteins, Neural networks, Neuron models of proteins, Protein interaction networks, Auto-associative neural networks, Hybrid models},
    abstract = {This study demonstrates the success of Auto-associative neural networks (ANNN) to represent protein networks, where each neuron maps to a protein and each neuron interaction to a specific protein interaction. Core mammalian cell cycle system with 12 proteins was used to train AANN with data generated from an ODE and Boolean models. When tested if AANN can find unknown system interactions, trained AANN with nonlinear (sigmoid) neurons captured accurate system dynamics but failed to capture the correct protein interactions. With correct protein interactions, AANN with linear neurons captured 50 % of protein behaviour and sigmoid AANN captured all protein dynamics correctly. This allowed hybrid-AANN with linear and nonlinear neurons. Self-learning ability of AANN was tested but it was not evident in the current model architecture. When tested for their ability to hold past memory by training AANN as a recurrent network, system dynamics revealed near perfect accuracy, with the network heavily relying on the past state to produce the current state. We also tested if neurons can be trained separately and assembled into AANN. Linear, nonlinear and binary (for representing Boolean) neurons were trained. Linear neurons modelled most proteins (70 %), and sigmoid neurons modelled all proteins correctly. Binary (perceptron) models successfully replicated Boolean rules of proteins. From these, a number of AANN models were assembled: sigmoid AANN accurately predicted the system; binary AANN revealed correct protein activation with temporal realism; two hybrid-AANN models, one with linear/sigmoid neuron models and another with binary/sigmoid neuron models, were successfully assembled to further simplify models.}
}

@incollection{MOVELLAN199110,
    title = {Contrastive Hebbian Learning in the Continuous Hopfield Model},
    editor = {David S. Touretzky and Jeffrey L. Elman and Terrence J. Sejnowski and Geoffrey E. Hinton},
    booktitle = {Connectionist Models},
    publisher = {Morgan Kaufmann},
    pages = {10-17},
    year = {1991},
    isbn = {978-1-4832-1448-1},
    doi = {https://doi.org/10.1016/B978-1-4832-1448-1.50007-X},
    url = {https://www.sciencedirect.com/science/article/pii/B978148321448150007X},
    author = {Javier R. Movellan},
    abstract = {This paper shows that contrastive Hebbian, the algorithm used in mean field learning, can be applied to any continuous Hopfield model. This implies that non-logistic activation functions as well as self connections are allowed. Contrary to previous approaches, the learning algorithm is derived without considering it a mean field approximation to Boltzmann machine learning. The paper includes a discussion of the conditions under which the function that contrastive Hebbian minimizes can be considered a proper error function, and an analysis of five different training regimes. An appendix provides complete demonstrations and specific instructions on how to implement contrastive Hebbian learning in interactive activation and competition models (a convenient version of the continuous Hopfield model).}
}