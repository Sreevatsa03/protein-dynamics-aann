\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{xcolor}
\setstretch{1.05}
\setlist[itemize]{topsep=2pt,itemsep=2pt,parsep=2pt}
\setlist[enumerate]{topsep=2pt,itemsep=2pt,parsep=2pt}
\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\itshape}

\begin{document}

\begin{center}
{\Large \textbf{Experimental Design Plan}}\\[4pt]
{\small For Project: \textit{Auto-Associative Neural Networks for Protein Interaction Dynamics}}\\
{\small Prepared by: Sreevatsa Nukala}
\end{center}

\section{Objectives and Success Criteria}

\textbf{Reproduction Goals}
\begin{itemize}
    \item R1: Sigmoid AANN with known signed connectivity fits trajectories accurately.
    \item R2: Linear AANN underfits; hybrid (linear + sigmoid) performs comparably to sigmoid.
    \item R3: Recurrent AANN improves temporal predictions; relies on past state.
    \item R4: ``Self-learning'' (no teacher forcing) drifts and accumulates error.
\end{itemize}

\textbf{Extension Goals}
\begin{itemize}
    \item X1: Analyze stability via Jacobians, fixed points, and eigenvalue spectra.
    \item X2: Implement and evaluate Contrastive Hebbian Learning (CHL) vs gradient descent.
    \item X3: Investigate unknown connectivity + sparsity (L1 regularization, identifiability).
    \item X4: Quantify memory contribution from recurrent terms.
    \item X5 (optional): Robustness and Hopfield-style energy basins.
\end{itemize}

Success = Complete all R1–R3 + any two of X1–X4 with figures and concise theoretical interpretation.

\section{Data Generation}

\subsection{Signed Wiring}
\begin{itemize}
    \item Signed adjacency $S \in \{-1, 0, +1\}^{12 \times 12}$ (from supplement).
    \item Sample magnitudes $|W|_{ij} \sim \text{LogNormal}(\mu_w=-1.2, \sigma_w=0.6)$ for $S_{ij}\neq 0$.
    \item $W_{\text{eff}} = S \odot |W|$, rescaled so $\rho(\alpha W_{\text{eff}}) \le 1$.
\end{itemize}

\subsection{Simulator A: Discrete-Time Sigmoid Network}
\[
x_{t+1} = \sigma\!\big(\alpha W_{\text{eff}}x_t + b + \eta_t\big),
\quad \eta_t \sim \mathcal{N}(0, \sigma_\eta^2 I)
\]
\begin{itemize}
    \item Parameters: $\alpha=0.9$, $\sigma_\eta=0.02$, $x_0 \sim \text{Beta}(2,2)^{12}$.
    \item Bias: $b = \sigma^{-1}(\mu) - \alpha W_{\text{eff}}\mu$, with $\mu \in [0.3,0.7]$.
\end{itemize}

\subsection{Simulator B: Continuous-Time ODE (Optional)}
\[
\dot{x} = -\tfrac{1}{\tau}x + \sigma(\alpha W_{\text{eff}}x + b) + \xi(t),
\quad \tau=1,\ \alpha=1
\]

\subsection{Simulator C: Binary Network (Optional)}
\[
x^{(B)}_{t+1} = \mathbb{1}\!\left[W^{(B)}x^{(B)}_t - \theta > 0\right]
\]
with thresholds $\theta_i$ matched to node in-degree.

\subsection{Data Structure and Splits}
\begin{itemize}
    \item Generate $T=400$ steps $\times$ 10 random seeds.
    \item Split: 70\% train / 15\% val / 15\% test (time-ordered).
    \item Produce:
    \begin{itemize}
        \item $\mathcal{D}^{(1)} = \{(x_t, x_{t+1})\}$ for non-recurrent.
        \item $\mathcal{D}^{(2)} = \{([x_t;x_{t-1}], x_{t+1})\}$ for recurrent.
        \item $\mathcal{D}^{(B)} = \{(x_t, y^{(B)}_{t+1})\}$ for binary.
    \end{itemize}
\end{itemize}

\section{Models}

\begin{itemize}
    \item \textbf{Non-recurrent AANN:} $y = \phi(Wx + b)$, with mask $M = \mathbb{1}[S\neq0]$.
    \item \textbf{Hybrid AANN:} per-node activations (sigmoid/linear) chosen by single-protein fits.
    \item \textbf{Recurrent AANN:} $x_{t+1} = \phi(Wx_t + Ux_{t-1} + b)$, $U$ diagonal or learned.
\end{itemize}

\section{Training Procedures}

\subsection{Manual NumPy Training (Non-recurrent)}
\begin{itemize}
    \item Loss: $\text{MSE}(y,x_{t+1})$.
    \item Gradients: $dz = 2(y - x_{t+1})/d \odot \phi'(z)$, $dW = dzx_t^\top \odot M$, $db = dz$.
    \item Optimizer: SGD + momentum ($\mu=0.9$, $\eta\in\{10^{-2},5\!\times\!10^{-3},10^{-3}\}$).
\end{itemize}

\subsection{Short Unrolled Recurrent Training}
\begin{itemize}
    \item Unroll $T=5$–10 steps, accumulate gradients, clip at 1.0.
    \item Optional PyTorch autograd if NumPy BPTT unstable.
\end{itemize}

\subsection{Contrastive Hebbian Learning (CHL)}
\begin{itemize}
    \item Free phase: converge to $r^F$ under input.
    \item Clamped phase: converge to $r^C$ under target.
    \item Update: $\Delta W = \eta (r^C r^{C\top} - r^F r^{F\top}) \odot M$, $\Delta b = \eta (r^C - r^F)$.
\end{itemize}

\subsection{Unknown Connectivity + L1 (ISTA)}
\[
W \leftarrow \text{soft}(W - \eta \nabla \text{MSE}, \eta \lambda),\quad
\lambda\in\{10^{-3},3\!\times\!10^{-3},10^{-2}\}
\]

\section{Experiments}

\subsection*{R1. Known-Graph Sigmoid vs Linear}
Compare MSE, correlations, overlays (3–4 proteins).  
\textit{Expected: Sigmoid $\ll$ Linear; reproduces paper’s main result.}

\subsection*{R2. Hybrid Model}
Per-node activation assignment from single-protein fits.  
\textit{Expected: Hybrid $\approx$ Sigmoid accuracy.}

\subsection*{R3. Recurrent Model}
Unroll 5–10 steps; ablate $U$.  
\textit{Expected: Recurrent outperforms non-recurrent; ablation increases error.}

\subsection*{R4. Self-Learning}
Roll model on own outputs without teacher forcing.  
\textit{Expected: Drift and instability.}

\subsection*{X1. Stability and Jacobian Analysis}
Compute $J = \text{diag}(\phi'(Wx^*+b))W$ at fixed point $x^*$.  
Analyze eigenvalues and stability fraction ($|\lambda|<1$).

\subsection*{X2. CHL vs SGD}
Compare convergence speed and final MSE.  
\textit{Expected: CHL slower, comparable final accuracy.}

\subsection*{X3. Unknown Connectivity + Sparsity}
Train with full $W$ and L1 penalty.  
Evaluate edge sign accuracy, precision/recall, AUROC.

\subsection*{X4. Memory Quantification}
Define Memory Index = $(\text{MSE}_{\text{nonrec}} - \text{MSE}_{\text{rec}})/\text{MSE}_{\text{nonrec}}$.  
Compute sensitivity via finite differences on $x_t$ vs $x_{t-1}$.

\subsection*{X5. Robustness (Optional)}
Inject weight/state noise; compute stability radius and visualize energy-like basins.

\section{Hyperparameters}

\begin{tabular}{ll}
\textbf{Parameter} & \textbf{Values / Defaults} \\ \hline
Simulator gain $\alpha$ & 0.8, \textbf{0.9}, 1.0 \\
Process noise $\sigma_\eta$ & 0.0, \textbf{0.02}, 0.04 \\
Bias mean $\mu$ & 0.4, \textbf{0.5}, 0.6 \\
LR & 1e--2, 5e--3, 1e--3 \\
Epochs & max 300, patience 20 \\
Unroll steps $T$ & 5, \textbf{8}, 10 \\
CHL step size $\eta$ & 1e--3, \textbf{3e--3}, 1e--2 \\
Seeds & 10 \\
\end{tabular}

\section{Metrics and Evaluation}

\begin{itemize}
    \item Primary: Test MSE, per-node correlation.
    \item Edge recovery: Precision, recall, AUROC vs ground truth $S$.
    \item Stability: $\%|\lambda|<1$, spectral radius of $J$.
    \item Statistical tests: paired $t$-test (sigmoid vs linear MSE).
\end{itemize}

\section{Implementation Layout}

\begin{verbatim}
src/
  data.py            # synthetic simulator
  numpy_core.py      # forward, grads, CHL, ISTA, Jacobians
  models.py          # AANN classes
  analysis.py        # stability, eigs, metrics
notebooks/
  01_repro_linear_vs_sigmoid.ipynb
  02_hybrid_and_single_protein.ipynb
  03_recurrent_and_self_learning.ipynb
  04_extensions_jacobian_chl_unknown.ipynb
\end{verbatim}

\section{Timeline}

\begin{itemize}
    \item \textbf{Day 1–2:} Data + R1 (Sigmoid vs Linear)
    \item \textbf{Day 3:} Hybrid + single-protein fits
    \item \textbf{Day 4:} Recurrent and memory analysis
    \item \textbf{Day 5:} Self-learning test + stability
    \item \textbf{Day 6:} CHL experiments
    \item \textbf{Day 7:} Unknown connectivity + sparsity
    \item \textbf{Day 8:} Memory quantification + robustness
\end{itemize}

\section{Risks and Mitigations}

\begin{itemize}
    \item \textbf{Gradient instability:} keep $T \le 10$; use gradient clipping.
    \item \textbf{CHL non-convergence:} use damping, small $\eta$; log $\Delta E$ trend.
    \item \textbf{Edge recovery failure:} sweep $\lambda$; accept ``prediction $\neq$ structure''.
\end{itemize}

\end{document}
